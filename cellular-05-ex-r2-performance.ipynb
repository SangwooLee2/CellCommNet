{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance evaluation purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created based on cellular-05-new-weekend-modified-r6-ISD.ipynb\n",
    "\n",
    "%reset -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import neighbors\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "import statsmodels.api as sm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# taken from https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/ \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM \n",
    "    # note: LSTM, if no activation is passed, then default seems to be sigmoid. https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "    # lsw) this doesn't seem to be tf.keras. Rather, this just seems to be keras. \n",
    "    \n",
    "from keras import optimizers\n",
    "from keras import callbacks    \n",
    "from itertools import cycle \n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_ = 108\n",
    "seed_ = 9\n",
    "np.random.seed(seed_)  # this is numpy\n",
    "random.seed(seed_) # this is python module \n",
    "\n",
    "print( np.random.get_state()[1][0] ) # lsw) At this moment, seed_ will be printed. But then, seed will keep changing \n",
    "\n",
    "drop_col_th = 0.2 # drop columns by this ratio of NA or null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "fname_part = now.strftime(\"%Y%m%d_%H%M%S\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# series_to_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, mode_=1):\n",
    "    # n_in: Number of lag observations as input (X). Values may be between [1..len(data)] Optional. Defaults to 1.\n",
    "    # n_out: Number of observations as output (y). Values may be between [0..len(data)-1]. Optional. Defaults to 1.    \n",
    "    # mode_ = 1 => var, var2,..., else: 'user', 'bytes'....\n",
    "    \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data) # convert to a dataframe \n",
    "    cols, names = list(), list() # empty list \n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        if mode_ == 1: \n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        else: \n",
    "            names += [('%s(t-%d)' % (c_name, i)) for c_name in list(data.columns) ]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if mode_ == 1: \n",
    "            if i == 0:\n",
    "                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        else: \n",
    "            if i == 0:\n",
    "                names += [('%s(t)' % (c_name)) for c_name in list(data.columns)]\n",
    "            else:\n",
    "                names += [('%s(t+%d)' % (c_name, i)) for c_name in list(data.columns)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dto_str_wo_yr_sec(dto_in):\n",
    "    # Given a datetime object as input, output a string w/ year, sec removed\n",
    "    # input: datetime object\n",
    "    # output: string\n",
    "    \n",
    "    return dto_in.strftime( \"%m-%d %H:%M\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter(string, substr): \n",
    "    import re\n",
    "    \n",
    "    return [str for str in string if\n",
    "             any(sub in str for sub in substr)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_rem_red(list_):\n",
    "    # remove redundancy from a list, while keeping order\n",
    "    u_list = [] # unique list\n",
    "    [u_list.append(item) for item in list_ if item not in u_list]\n",
    "    \n",
    "    return u_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(list1, list2):\n",
    "    out = [item for item in list1 if not item in list2]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = float(input('Enter threshold for num_signchange(0-1):  '))\n",
    "\n",
    "def num_signchange(x):\n",
    "    # x is a series \n",
    "    \n",
    "    if x.empty:\n",
    "        return None\n",
    "    else: \n",
    "        # https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python/21171725\n",
    "        y = np.array(x)\n",
    "        y_ = np.array(y[1:]) - np.array(y[0:-1])\n",
    "        pos1 = np.where(np.diff(np.signbit( y_ )))[0] # find where sign changes occur, not the latter posion but the front position \n",
    "        pos2 = np.nonzero( np.abs(y_)> thre )[0]  # assuming minmax scaled, I assumed max 1 x 0.2 as the slope criterion\n",
    "\n",
    "        return len( np.intersect1d(pos1, pos2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Used for df_traffic_set (subset around t_bs_i), EDA, performance estimation, plot df_traffic_stat_geo\n",
    "###################################################################\n",
    "    \n",
    "adj_bs_choice = 3 # int(input('Enter adj bs choice for bs_set/df_traffic_set and then LSTM, 1(no adj bs), 2(with max_n_adj_bs bs), 3(input BS index manually): '))\n",
    "\n",
    "max_n_adj_bs = 7 # int(input('Enter max_n_adj_bs(default 7) for reading or calculation purpose(incl center cell):  '))\n",
    "max_n_adj_bs_rest = -1 # int(input('(for df_traffic_set) To adopt # of adj bs is smaller than max_n_adj_bs, enter the number, else enter -1(incl center cell):  '))\n",
    "    \n",
    "input_choice = 1 # int( input('Input(used for df_traffic_set generation), 1(users), 2(pkt), 3(bytes), 4(users, pkt), 5(users, bytes), 6(pkt, bytes), 7(all):  ') )\n",
    "\n",
    "wknd_in_input = 2 # int(input('Include weekend info in the input_col? Enter 1 to do, 2 not to:  '))\n",
    "\n",
    "pred_target_choice = 1 # int( input('Predict target(sh be subset of input_col), 1(users), 2(pkt), 3(bytes), 4(users, pkt), 5(users, bytes), 6(pkt, bytes), 7(all):  ') )\n",
    "\n",
    "read_df_dist_m = 1 # int(input('Enter 1 to read df_dist_m and df_adj_bs, 2 to calc and save:  '))\n",
    "\n",
    "DoKmeansCluster = 2 # int(input('Enter 1 to do Kmeans clustering, 2 to load previous results, 3 to load tdomain clustering: '))  \n",
    "    \n",
    "Do_perf_est = 1 # int(input('Enter 1 to do LSTM evaluation, 2 not to:  '))    \n",
    "\n",
    "tr_ratio = 0.7 # float(input('Enter training ratio(def:0.7):  '))\n",
    "\n",
    "# parameter input for reframing \n",
    "n_in_time = int(input('Enter n_in_time(>1 for multiple lag input):  ')) # 1 means, (t-1) only, 2 means (t-2) and (t-1)\n",
    "n_out_time = int(input('Enter n_out_time:  ')) # 1 means t only, 2 means t and t+1\n",
    "\n",
    "use_early_stop = 2 # int(input('Enter 1 to use early stopping:  '))\n",
    "\n",
    "cl_i = int(input('Enter cluster index for analysis: '))\n",
    "\n",
    "bs_list_cl_sset_sz = int(input('Enter bs_list_cl_sset_sz:  '))\n",
    "\n",
    "n_iter = int(input('Enter n_iter for LSTM iteration(default is 5): '))\n",
    "learn_rate_ = 0.001 # float(input('Enter learning rate(def is 0.001:  '))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch_lst = [ ['NULL'], ['users'], ['packets'], ['bytes'], ['users', 'packets'], ['users', 'bytes'], ['packets', 'bytes'], ['users', 'packets', 'bytes'] ]\n",
    "pred_tg_ch_lst = input_ch_lst.copy()\n",
    "\n",
    "input_col = input_ch_lst[input_choice].copy()\n",
    "\n",
    "if wknd_in_input==1:\n",
    "    input_col +=['weekend']\n",
    "\n",
    "pred_target_col = pred_tg_ch_lst[pred_target_choice]\n",
    "\n",
    "shorten_bs = 1 # int(input('Enter 1 to restrict bs list by longitude/latitude range:  '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans clustering #1 (simple), based on df_traffic_FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DoKmeansCluster == 2:\n",
    "    pkl_name = 'kmeans__hvch_2_clsz_9_stch_9_20210315_003517.pkl' # input('Enter pkl_name with data path: ')\n",
    "    df_traffic_FE_cl_name = 'df_traffic_FE_hvch_2_clsz_9_stch_9_20210315_003517.csv' # input('Enter df_traffic_FE_cl_name with path:  ')\n",
    "        \n",
    "    # https://stackoverflow.com/questions/54879434/how-to-use-the-pickle-to-save-sklearn-model\n",
    "    kmeans = pickle.load(open(pkl_name, \"rb\"))\n",
    "    df_traffic_FE_cluster = pd.read_csv(df_traffic_FE_cl_name, header=0)\n",
    "    \n",
    "    print('\\n ====== kmeans.cluster_centers_ ======')\n",
    "    print(kmeans.cluster_centers_[:, 0:10])\n",
    "    print(kmeans.cluster_centers_.shape)\n",
    "\n",
    "    print('\\n kmeans.labels_[0:10]')\n",
    "    print(kmeans.labels_[0:15])\n",
    "    print( pd.Series( kmeans.labels_).value_counts() )\n",
    "    \n",
    "elif DoKmeansCluster == 3: # load tdomain clustering \n",
    "    # df_traffic_FE_cluster = pd.read_csv(\"AlexM_KMClust_7_dftrafficsc_users_r1.csv\", header=0)\n",
    "    df_traffic_FE_cluster_fname = input('Enter df_traffic_FE_cluster_fname including path:  ')\n",
    "    df_traffic_FE_cluster = pd.read_csv(df_traffic_FE_cluster_fname, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_list_cl = list( df_traffic_FE_cluster[df_traffic_FE_cluster.cluster==cl_i].bs )\n",
    "\n",
    "np.random.seed(seed_)  # this is numpy # do it again, so that .choice() is reproducible\n",
    "\n",
    "if cl_i == 0:\n",
    "    bs_list_cl_sset = [6684, 5713, 3255, 4008]\n",
    "elif cl_i == 1:     \n",
    "    # bs_list_cl_sset = [4180, 8827, 3789, 8512] \n",
    "    bs_list_cl_sset = [4180, 3789, 8512, 3192] \n",
    "elif cl_i == 2:\n",
    "    bs_list_cl_sset = [7198, 7749, 7778, 6277]\n",
    "elif cl_i == 3:\n",
    "    bs_list_cl_sset = [     ]   \n",
    "elif cl_i == 4:\n",
    "    bs_list_cl_sset = [5317, 5318, 5880, 6042]\n",
    "elif cl_i == 5:\n",
    "    bs_list_cl_sset = [7016, 6032, 6573, 7664]        \n",
    "elif cl_i == 6:        \n",
    "    bs_list_cl_sset = [3098, 1059, 11382, 4515]\n",
    "elif cl_i == 7: \n",
    "    bs_list_cl_sset = [      ]      \n",
    "elif cl_i == 8:        \n",
    "    bs_list_cl_sset = [3110, 8349, 8482, 13110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_traffic and df_toplogy are read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime.datetime.fromtimestamp( int(x) ) => returns ex) datetime.datetime(2012, 8, 19, 1, 0), a datetime object \n",
    "# x: numpy.int64 object \n",
    "    \n",
    "def parse03(x):\n",
    "    return datetime.datetime.fromtimestamp( int(x) )\n",
    "        # returned example: 2012-08-19 01:00:00\n",
    "        # Otherwise, directly from the csv file, time_hour column would be like: ex) 1345305600\n",
    "        # Looks like, int(x) works same as just x \n",
    "        # datetime.fromtimestamp: Return the local date and time corresponding to the POSIX timestamp, \n",
    "        # such as is returned by...\n",
    "        # csv file's Time_hour definition: hourly timestamp in UNIX epoch time (time zone GMT+8).\n",
    "\n",
    "df_traffic = pd.read_csv(\"cellular_traffic_full.csv\", \\\n",
    "                     parse_dates = [1],date_parser = parse03, header=0)\n",
    "\n",
    "df_topology = pd.read_csv(\"topology.csv\", header = 0, index_col=0)\n",
    "    # note) longitude, latitude: usually units are in degrees # bs_i works as index    \n",
    "    \n",
    "print( df_traffic.describe() )\n",
    "print('\\n')\n",
    "print( df_topology.describe() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_dist_m, df_adj_bs generated, using df_topology\n",
    "## of bs x max_n_adj_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "# The answers above are based on the Haversine formula, which assumes the earth is a sphere, \n",
    "# which results in errors of up to about 0.5% (according to help(geopy.distance)). \n",
    "# Vincenty distance uses more accurate ellipsoidal models such as WGS-84, and is implemented in geopy. \n",
    "\n",
    "# Update: 04/2018: Note that Vincenty distance is deprecated since GeoPy version 1.13 - you should use geopy.distance.distance() instea\n",
    "\n",
    "def my_geopy_dist_m(lat1, long1, lat2, long2):\n",
    "    # return geopy.distance.vincenty([lat1,long1], [lat2, long2]).m\n",
    "    return geopy.distance.distance([lat1,long1], [lat2, long2]).m\n",
    "\n",
    "# https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "def my_geopy_dist_list2_m(lat1, long1, lat2, long2):\n",
    "    # lat1, long1, lat2, long2: should be in radians \n",
    "    R_km = 6371\n",
    "    diff_long = np.radians(long2 - long1)\n",
    "    diff_lat = np.radians(lat2 - lat1)\n",
    "    a = math.sin(diff_lat/2)**2 + math.cos(np.radians(lat1)) * math.cos(np.radians(lat2)) * math.sin(diff_long/2)**2\n",
    "    b = 2 * math.asin( np.minimum(1, np.sqrt(a))) \n",
    "    d = R_km * b\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether adj_bs_choice == 1 or 2, we setup assuming  = 7 here \n",
    "\n",
    "header_ = []\n",
    "for ii in range(0,max_n_adj_bs-1): # -1, since max_n_adj_bs includes center cell\n",
    "    header_ = header_ + ['adj_bs_i_' + str(ii)]\n",
    "    \n",
    "if read_df_dist_m==2: # then calculate     \n",
    "    df_dist_m = pd.DataFrame(index=df_topology.index, columns=header_) # bs_i works as index\n",
    "    df_adj_bs = pd.DataFrame(index=df_topology.index, columns=header_) # bs_i works as index   \n",
    "elif read_df_dist_m==1: # then read\n",
    "    if max_n_adj_bs ==7:\n",
    "        df_dist_m = pd.read_csv( 'df_dist_m.csv', index_col=0, header = 0)\n",
    "        df_adj_bs = pd.read_csv( 'df_adj_bs.csv', index_col=0, header = 0)\n",
    "    else: \n",
    "        df_dist_m = pd.read_csv( 'df_dist_m_'+str(max_n_adj_bs)+'.csv' , index_col=0, header = 0)\n",
    "        df_adj_bs = pd.read_csv( 'df_adj_bs_'+str(max_n_adj_bs)+'.csv' , index_col=0, header = 0)\n",
    "        \n",
    "    df_dist_m = df_dist_m.iloc[:, 0:max_n_adj_bs-1] # ex) if max_n_adj_bs=7, 0:6 ==> means neighboring 6 cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_df_dist_m==2: # then calculate \n",
    "    start_time = datetime.datetime.now() \n",
    "\n",
    "    for bs_i in df_topology.index: \n",
    "        # for bs_i in range(1,2):\n",
    "        lon_ = float( df_topology.loc[ df_topology.index==bs_i].lon )\n",
    "        lat_ = float( df_topology.loc[ df_topology.index==bs_i].lat )\n",
    "        lon_rep = np.repeat(lon_, df_topology.shape[0]-1)\n",
    "        lat_rep = np.repeat(lat_, df_topology.shape[0]-1)\n",
    "        lon_exc = list( df_topology.loc[ df_topology.index!=bs_i].lon )\n",
    "        lat_exc = list( df_topology.loc[ df_topology.index!=bs_i].lat )\n",
    "\n",
    "        dist_m_vec = list( map( my_geopy_dist_m, lat_rep, lon_rep, lat_exc, lon_exc) ) # => takes 2.8 sec \n",
    "            # or use: dist_m_vec = [ geopy.distance.distance([x,y], [z, w]).m for x,y,z,w in zip( lat_rep, lon_rep, lat_exc, lon_exc ) ]\n",
    "        adj_bs_vec = list(  set(df_topology.index)-set([bs_i])  ) \n",
    "            # using set and not being able to preserve order is OK here. \n",
    "        \n",
    "        idx = np.argsort( dist_m_vec ) # sort in ascending order => then find the idx\n",
    "        idx = idx[0:max_n_adj_bs-1] # take only max_n_adj_bs elements # -1 since, max_n_adj_bs means including center cell \n",
    "\n",
    "        dist_m_vec = [dist_m_vec[i] for i in idx] # take only max_n_adj_bs -1 elements \n",
    "        adj_bs_vec = [adj_bs_vec[i] for i in idx] # take only max_n_adj_bs -1 elements \n",
    "\n",
    "        df_dist_m.loc[ df_dist_m.index==bs_i,:]=dist_m_vec\n",
    "        df_adj_bs.loc[ df_adj_bs.index==bs_i,:]=adj_bs_vec\n",
    "\n",
    "        if bs_i % 30 == 0: \n",
    "            time_elapsed = datetime.datetime.now() - start_time # start_time is fixed, was measured at the point of loop_i=0, time_elapsed keeps being updated\n",
    "            print('Avg time consumed per iteration(hh:mm:ss.ms) is {}'.format(time_elapsed/(bs_i)) )    \n",
    "            print('Time elapsed so far(hh:mm:ss.ms) is %s as of %s'%(format(time_elapsed),datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "            print('Remaiging time estimated(hh:mm:ss.ms) is %s as of %s' %(format(time_elapsed/(bs_i)*( max(df_topology.index)-bs_i )),datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) )   \n",
    "            \n",
    "    if max_n_adj_bs==7:\n",
    "        df_dist_m.to_csv( 'city-cellular-traffic-map-master/traceset/df_dist_m.csv')\n",
    "        df_adj_bs.to_csv( 'city-cellular-traffic-map-master/traceset/df_adj_bs.csv')\n",
    "    else: \n",
    "        df_dist_m.to_csv( 'city-cellular-traffic-map-master/traceset/df_dist_m_' + str(max_n_adj_bs) +'.csv')\n",
    "        df_adj_bs.to_csv( 'city-cellular-traffic-map-master/traceset/df_adj_bs_' + str(max_n_adj_bs) +'.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_traffic is further processed to incorporate weekend info or ISD info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic['weekend']=list( (df_traffic.time_hour.dt.weekday==5) | (df_traffic.time_hour.dt.weekday==6) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_info generation for df_traffic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique =df_traffic.nunique() \n",
    "null_perc = df_traffic.isnull().sum()/df_traffic.shape[0]*100\n",
    "NA_perc = df_traffic.isnull().sum()/df_traffic.shape[0]*100\n",
    "\n",
    "df_info = pd.DataFrame({'n_unique': n_unique, 'null_perc': null_perc, 'NA_perc': NA_perc})\n",
    "df_info[\"dtyp\"]=\"\"\n",
    "df_info[\"iotyp\"]=\"\"\n",
    "\n",
    "for r_i in range(0, df_info.shape[0] ):\n",
    "    if df_info['n_unique'][r_i] >=10: # 10: just chosen \n",
    "        df_info['dtyp'][r_i] = 'numeric'\n",
    "    elif df_info['n_unique'][r_i] ==2: # don't care, whether nominal or ordinal \n",
    "        df_info['dtyp'][r_i] = 'categorical'\n",
    "    else:\n",
    "        df_info['dtyp'][r_i] = 'nominal' \n",
    "        \n",
    "    df_info[\"iotyp\"][r_i]='inp'  \n",
    "        \n",
    "print(df_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS shortening\n",
    "## df_topology, df_traffic are affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shorten_bs == 1:\n",
    "    lon_min = df_topology.describe().loc['mean','lon']-0.5*df_topology.describe().loc['std','lon']\n",
    "    lon_max = df_topology.describe().loc['mean','lon']+0.5*df_topology.describe().loc['std','lon']\n",
    "    lat_min = df_topology.describe().loc['mean','lat']-0.5*df_topology.describe().loc['std','lat']\n",
    "    lat_max = df_topology.describe().loc['mean','lat']+0.5*df_topology.describe().loc['std','lat']\n",
    "    \n",
    "    print(df_topology.shape)\n",
    "    \n",
    "    df_topology = df_topology.loc[(df_topology['lon'] >= lon_min) & (df_topology['lon'] <= lon_max) & \\\n",
    "                    (df_topology['lat'] >= lat_min) & (df_topology['lat'] <= lat_max)]\n",
    "    \n",
    "    df_traffic = df_traffic.loc[ df_traffic.bs.isin(list(df_topology.index)) ]    \n",
    "    \n",
    "    print(df_topology.shape)\n",
    "    print('\\n')\n",
    "    print( df_traffic.describe() )\n",
    "    print('\\n')\n",
    "    print( df_topology.head(3) )\n",
    "    print('\\n')    \n",
    "    print( df_topology.describe() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shorten_bs == 1:\n",
    "    print(\"================ df_traffic.head(3) ================\")\n",
    "    print(df_traffic.head(3))\n",
    "\n",
    "    print(\"\\n================ df_traffic.shape ================\")\n",
    "    print(df_traffic.shape)\n",
    "\n",
    "    print(\"\\n================ df_traffic.dtypes ================\")\n",
    "    print(df_traffic.dtypes)\n",
    "\n",
    "    print(\"================ df_topology.head(3) ================\")\n",
    "    print(df_topology.head(3))\n",
    "\n",
    "    print(\"\\n================ df_topology.shape ================\")\n",
    "    print(df_topology.shape)\n",
    "\n",
    "    print(\"\\n================ df_topology.dtypes ================\")\n",
    "    print(df_topology.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "## df_traffic_set (subset around t_bs_i) generated using bs_set\n",
    "## There is NA within df_traffic_set, since starting time differs for BSs ==> Need to handle this.\n",
    "#########################################################################################################\n",
    "\n",
    "missing_proc = 1 # int(input('How to process missing values? 1(dropna), 2(fillna -1), 3(masking)'))\n",
    "\n",
    "temp1 = np.tile( ['time_'], (1, n_out_time*len(pred_target_col) )  )[0]\n",
    "temp2 = np.tile( np.array(range(0, n_out_time)) , (1,len(pred_target_col)) )[0]\n",
    "temp3 = np.tile( ['_target_'], (1, n_out_time*len(pred_target_col) )  )[0]\n",
    "temp4 = np.tile( np.array( [list( range(0, len(pred_target_col)) )] ).transpose(), (1, n_out_time) ).reshape(-1, len(pred_target_col)*n_out_time )[0]\n",
    "rmse_c = ['bs1', 'bs2']\n",
    "for ii in range(0,len(temp1)):\n",
    "    rmse_c.append( temp1[ii]+str(temp2[ii])+temp3[ii]+str(temp4[ii]) )\n",
    "del temp1, temp2, temp3, temp4\n",
    "df_rmse_arr_avg = pd.DataFrame(columns = rmse_c) \n",
    "df_rmse_arr_nsc_avg = pd.DataFrame(columns = rmse_c) \n",
    "\n",
    "start_time2 = datetime.datetime.now()\n",
    "\n",
    "n_neuron = int(input('Enter # of neurons for LSTM(default 50):  '))\n",
    "batch_size_ = int(input('Enter batch_size(def:288)'))\n",
    "n_epochs = int(input('Enter n_epochs(default 100): '))\n",
    "\n",
    "###################################\n",
    "for bs_ii in range(0, len(bs_list_cl_sset) ):\n",
    "###################################    \n",
    "    for bs_jj in range(0, len(bs_list_cl_sset) ):\n",
    "        t_bs_i = bs_list_cl_sset[bs_ii] # cell where performance needs to be evaluated\n",
    "        if bs_ii == bs_jj: \n",
    "            bs_set = [ bs_list_cl_sset[bs_ii] ]\n",
    "        else: \n",
    "            bs_set = [ bs_list_cl_sset[bs_ii] , bs_list_cl_sset[bs_jj] ]\n",
    "        \n",
    "        col_names = []\n",
    "        idx_ = 0\n",
    "\n",
    "        for bs_i in bs_set:\n",
    "            if bs_i==bs_set[0]:\n",
    "                df_traffic_set = df_traffic.loc[df_traffic.bs==bs_i, ['time_hour'] + input_col ]\n",
    "                col_names = col_names + ['time_hour'] + list( map( lambda a,b: a+b, input_col , ['_' + str(bs_i)]*len(input_col) ) )\n",
    "            else:\n",
    "                idx_ = idx_ + 1        \n",
    "                \n",
    "                df_traffic_set = pd.merge(df_traffic_set, df_traffic.loc[df_traffic.bs==bs_i, ['time_hour']+input_col ] \\\n",
    "                                          , how='outer', on ='time_hour' )\n",
    "                col_names = col_names + list( map( lambda a,b: a+b, input_col, ['_' + str(bs_i)]*len(input_col) ) )\n",
    "            \n",
    "        df_traffic_set = df_traffic_set.sort_values(by = 'time_hour', axis=0, ascending=True ) # otherwise, merge was unsorted sometimes     \n",
    "        df_traffic_set.columns = col_names # use the completed col_names\n",
    "\n",
    "        null_perc = df_traffic_set.isnull().sum()/df_traffic_set.shape[0]*100\n",
    "        NA_perc = df_traffic_set.isnull().sum()/df_traffic_set.shape[0]*100\n",
    "\n",
    "            \n",
    "        null_perc_set = df_traffic_set.isnull().sum()/df_traffic_set.shape[0]*100\n",
    "        print(\"================= before droping columns =============== \")\n",
    "        print( null_perc_set )\n",
    "        print( df_traffic_set.shape )\n",
    "\n",
    "        df_traffic_set = df_traffic_set.iloc[:, list(null_perc_set < drop_col_th*100 ) ]\n",
    "\n",
    "        null_perc_set = df_traffic_set.isnull().sum()/df_traffic_set.shape[0]*100\n",
    "        print(\"================= after droping columns =============== \")    \n",
    "        print( null_perc_set )\n",
    "        print( df_traffic_set.shape )        \n",
    "        \n",
    "        df_traffic_set = df_traffic_set.set_index('time_hour') # W/o this command, interpolate() encounters errors \n",
    "        interp_number = 20 # int(input('Enter interp_number:  '))\n",
    "\n",
    "        df_traffic_set = df_traffic_set.resample(str(interp_number)+'T').interpolate()\n",
    "        # ref for resample: https://machinelearningmastery.com/resample-interpolate-time-series-data-python/ \n",
    "        # T works as one minte # '3T': 3 minutes# '30S': 30 seconds \n",
    "\n",
    "        if missing_proc == 1:\n",
    "            df_traffic_set = df_traffic_set.dropna(axis=0, how='any') # row is remoed by any \n",
    "        elif missing_proc == 2:\n",
    "            df_traffic_set.fillna(-1000, inplace=True)\n",
    "        elif missing_proc == 3:    \n",
    "            df_traffic_set.fillna(-1000, inplace=True)\n",
    "\n",
    "        index_ = list(df_traffic_set.index)\n",
    "        columns_ = list(df_traffic_set.columns)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        df_traffic_set = pd.DataFrame(data = scaler.fit_transform( df_traffic_set.values ), index=index_, columns=columns_) # bs_i works as index\n",
    "        del index_\n",
    "        del columns_\n",
    "        \n",
    "        df_traffic_set_ref = series_to_supervised( df_traffic_set, n_in_time, n_out_time, True, 2 ) # shape is (43799, 16) # a dataframe \n",
    "        \n",
    "        var_list_in3 = [x for x in list(df_traffic_set_ref.columns) if 't-' in x]\n",
    "\n",
    "        dd = [elem in pred_target_col for elem in input_col] # ex) [True, True, False]\n",
    "        dd_idx = [i+1 for i in range(len(dd)) if dd[i] == True] # make it a list of index # +1 so that 1,2,3 instead of 0,1,2\n",
    "        dd_idx_str = [\"{}\".format(element) for element in dd_idx] # ex) ['1', '2', '3']\n",
    "\n",
    "        var_list_out3 = [x for x in list(df_traffic_set_ref.columns) if ('t-' not in x)]\n",
    "        var_list_out3 = Filter(var_list_out3, pred_target_col) # out of var_list_out3, take only ones contraiing elements of pred_target_col\n",
    "        var_list_out3 = Filter(var_list_out3, [str(t_bs_i)] )\n",
    "        \n",
    "        drop_pos = list( set(df_traffic_set_ref.columns) - set(var_list_in3) - set(var_list_out3) )\n",
    "\n",
    "        df_traffic_set_ref.drop( drop_pos, axis=1, inplace=True ) \n",
    "        \n",
    "        n_train_samples = round(df_traffic_set_ref.shape[0]*tr_ratio)\n",
    "\n",
    "        train = df_traffic_set_ref.iloc[:n_train_samples, :] # both x and y \n",
    "        test = df_traffic_set_ref.iloc[n_train_samples:, :] # both x an y \n",
    "       \n",
    "        train_X, train_y = train[var_list_in3], train[var_list_out3]\n",
    "        test_X, test_y = test[var_list_in3], test[var_list_out3]     \n",
    "        \n",
    "        train_X = train_X.values.reshape(( train_X.shape[0], n_in_time, len(input_col)*len(bs_set) ))\n",
    "        test_X = test_X.values.reshape((   test_X.shape[0],  n_in_time, len(input_col)*len(bs_set) ))        \n",
    "        \n",
    "        stacked_LSTM = 1 # int(input('Enter 1 for one additiional stacked LSTM layer, 2 for two additinal LSTM layers:  '))\n",
    "        if stacked_LSTM == 1:\n",
    "            n_neuron_stacked = 20 # int(input('n_neuron_stacked:  '))\n",
    "        elif stacked_LSTM == 2:\n",
    "            n_neuron_stacked = int(input('n_neuron_stacked:  '))\n",
    "            n_neuron_stacked2 = int(input('n_neuron_stacked2:  '))\n",
    "\n",
    "        acti_fn = 1# int(input('enter 1 for sigmoid, 2 for relu: '))\n",
    "        if acti_fn == 1:\n",
    "            acti_fn = 'sigmoid'\n",
    "        elif acti_fn == 2:\n",
    "            acti_fn = 'relu'\n",
    "        \n",
    "        rmse_arr = np.zeros(( n_iter, n_out_time, len(pred_target_col)) )\n",
    "        rmse_arr_nsc  = np.zeros(( n_iter, n_out_time, len(pred_target_col)) )\n",
    "        \n",
    "        ###################################\n",
    "        for iter_ii in range(0,n_iter):\n",
    "        ###################################    \n",
    "            # design network\n",
    "            model = Sequential()\n",
    "            if stacked_LSTM in [1]: # https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/ => stacked LSTM\n",
    "                model.add(LSTM(n_neuron, activation=acti_fn, return_sequences=True, \\\n",
    "                               input_shape=(train_X.shape[1], train_X.shape[2]))) \n",
    "                    # [ timesteps, features]. 3 50 neurons\n",
    "                    # or n_in_time, len(input_col) \n",
    "                    # 50: a hidden layer with 4 LSTM blocks or neurons,    \n",
    "                    # return_sequences=True : https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/ => stacked LSTM\n",
    "                model.add( LSTM(n_neuron_stacked, activation=acti_fn) )\n",
    "                model.add(Dense(n_out_time))\n",
    "            elif stacked_LSTM in [2]:   \n",
    "                # https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "                model.add(LSTM(n_neuron, activation=acti_fn, return_sequences=True, \\\n",
    "                               input_shape=(train_X.shape[1], train_X.shape[2]))) \n",
    "                model.add( LSTM(n_neuron_stacked, activation=acti_fn, return_sequences=True) )\n",
    "                model.add( LSTM(n_neuron_stacked2, activation=acti_fn) )\n",
    "                model.add(Dense(n_out_time))\n",
    "\n",
    "            else: \n",
    "                model.add( LSTM(n_neuron, activation=acti_fn, input_shape=(train_X.shape[1], train_X.shape[2])) ) \n",
    "                    #  [ timesteps, features]. 3 50 neurons\n",
    "                    # or n_in_time, len(input_col) \n",
    "                    # 50: a hidden layer with 4 LSTM blocks or neurons,\n",
    "                model.add(Dense(n_out_time))\n",
    "\n",
    "            # https://keras.io/api/optimizers/adam/\n",
    "                # tf.keras.optimizers.Adam( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name=\"Adam\", **kwargs)\n",
    "            # https://datascience.stackexchange.com/questions/48736/how-to-see-change-learning-rate-in-keras-lstm\n",
    "                # sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "                # model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "            adam_ = optimizers.Adam(lr = learn_rate_ )\n",
    "            model.compile( loss='mae', optimizer=adam_ )\n",
    "\n",
    "            # fit network\n",
    "            es_callback = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "            if use_early_stop==1: \n",
    "                # https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323\n",
    "                history = model.fit(train_X, train_y, epochs=n_epochs, batch_size=batch_size_, \\\n",
    "                                validation_data=(test_X, test_y), verbose=1, shuffle=False, callbacks=[es_callback] )   \n",
    "            else: \n",
    "                history = model.fit(train_X, train_y, epochs=n_epochs, batch_size=batch_size_, \\\n",
    "                                validation_data=(test_X, test_y), verbose=1, shuffle=False )\n",
    "\n",
    "            # plot history # https://keras.io/ko/visualization/ or \n",
    "            pyplot.plot(history.history['loss'], label='train')\n",
    "            pyplot.plot(history.history['val_loss'], label='test')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            pyplot.legend()\n",
    "            pyplot.show()\n",
    "\n",
    "            test_y_hat = model.predict(test_X) # 2D ndarray is returned\n",
    "\n",
    "            test_X_resh = test_X.reshape((test_X.shape[0], n_in_time*len(input_col)*len(bs_set) )) # 3D => 2D \n",
    "            # back to 2D dataframe \n",
    "            test_X_resh = pd.DataFrame(data = test_X_resh, columns=var_list_in3 ) # bs_i works as index\n",
    "            test_y_hat  = pd.DataFrame(data=test_y_hat, columns = var_list_out3 )\n",
    "\n",
    "\n",
    "            for t_i in range(0,n_out_time): # time index\n",
    "                for v_i in range(0, len(pred_target_col)): # variable index\n",
    "                    input_1   = test_X_resh.iloc[ :, 0:len(input_col)*len(bs_set) ] \n",
    "                    input_1_1 = test_X_resh.iloc[ :, 0:len(input_col)*len(bs_set) ] \n",
    "\n",
    "                    # from test_y_hat => make a column \n",
    "                    if t_i == 0: \n",
    "                        input_2   = test_y_hat[  [pred_target_col[v_i]+'_'+str(t_bs_i)+'(t)'] ]\n",
    "                        input_2_1 = list( test_y[ pred_target_col[v_i]+'_'+str(t_bs_i)+'(t)' ] )\n",
    "                            # Why list()? Othewise, input_2_1 would be a dataframe with time as index, unlike input_2\n",
    "                            # Then, plugging into input_1_1 will cause problem \n",
    "                    else: \n",
    "                        input_2   = test_y_hat[  [pred_target_col[v_i]+'_'+str(t_bs_i)+'(t+' + str(t_i) + ')'] ]\n",
    "                        input_2_1 = list( test_y[ pred_target_col[v_i]+'_'+str(t_bs_i)+'(t+' + str(t_i) + ')' ] )\n",
    "                            # othewise, input_2_1 would be a dataframe with time as index, unlike input_2\n",
    "                            # Then, plugging into input_1_1 will cause problem \n",
    "\n",
    "                    input_1.iloc[:, dd_idx[v_i]-1]   = input_2   # -1 since, dd_idx was generated with +1 \n",
    "                    input_1_1.iloc[:, dd_idx[v_i]-1] = input_2_1 # -1 since, dd_idx was generated with +1 \n",
    "\n",
    "                    inv_test_y_hat = scaler.inverse_transform(input_1) # in/out would be same size as df_traffic_set\n",
    "                    inv_test_y_hat = inv_test_y_hat[:, dd_idx[v_i]-1 ] # slice out only this position\n",
    "\n",
    "                    inv_test_y     = scaler.inverse_transform(input_1_1) # in/out would be same size as df_traffic_set\n",
    "                    inv_test_y     = inv_test_y[:, dd_idx[v_i]-1 ] # slice out only this position\n",
    "\n",
    "                    rmse_arr[iter_ii, t_i, v_i] = np.sqrt(sklearn.metrics.mean_squared_error(inv_test_y, inv_test_y_hat))\n",
    "                    rmse_arr_nsc[iter_ii, t_i, v_i] = np.sqrt(sklearn.metrics.mean_squared_error(input_2, input_2_1))                    \n",
    "                    print('Test RMSE: %.3f' % rmse_arr[iter_ii, t_i, v_i] ) \n",
    "\n",
    "                    if iter_ii==0 and t_i == n_out_time-1: ## plot only once\n",
    "                        t_axis =  df_traffic_set_ref.index[n_train_samples:]\n",
    "                        plt.plot(t_axis, inv_test_y, 'r', label='inv_test_y') # plotting t, a separately \n",
    "                        plt.plot(t_axis, inv_test_y_hat, 'b', label='inv_test_y_hat') # plotting t, b separately \n",
    "\n",
    "                        title_ =  'var_' + pred_target_col[v_i] + '(t+' + str(t_i) + ')'\n",
    "\n",
    "                        plt.title(title_)\n",
    "                        plt.legend()\n",
    "                        plt.show()       \n",
    "\n",
    "                    if iter_ii==0 and t_i==n_out_time-1: ## plot only once\n",
    "                        t_axis =  df_traffic_set_ref.index[n_train_samples:]\n",
    "\n",
    "                        plt.plot(t_axis, test_y.iloc[:, [t_i*len(pred_target_col)+v_i]], 'r', label='test_y(t)') # plotting t, a separately \n",
    "                        plt.plot(t_axis, test_y_hat.iloc[:, [t_i*len(pred_target_col)+v_i]], 'b', label='test_y_hat(t)') # plotting t, b separately \n",
    "     \n",
    "                        temp1 = test_y_hat.iloc[ test_y_hat.shape[1]-1:, [0] ] # (t) with delay\n",
    "                        temp2 = np.zeros( [test_y_hat.shape[1]-1,1] ) \n",
    "\n",
    "                        plt.plot(t_axis, np.concatenate( (temp1, temp2), axis= 0 )    , 'k', label='test_y_hat(t+delay)') # plotting t, b separately \n",
    "\n",
    "                        title_ =  'var_' + pred_target_col[v_i] + '_shifted'\n",
    "                        plt.title(title_)\n",
    "                        plt.legend()\n",
    "                        plt.show()                  \n",
    "\n",
    "            # calculated and printed per iter_ii\n",
    "            time_elapsed2 = datetime.datetime.now() - start_time2 # start_time is fixed, was measured at the point of loop_i=0, time_elapsed keeps being updated\n",
    "            if bs_jj - (bs_ii-1) != 0:\n",
    "                avg_time = time_elapsed2/(iter_ii+1)/(bs_ii+1)/(bs_jj - (bs_ii-1))\n",
    "                rem_time = avg_time * ( len(bs_list_cl_sset)-1 - (bs_ii-1) ) * ( len(bs_list_cl_sset)-1 - (bs_jj-1) )*( n_iter-1 - (iter_ii-1) )\n",
    "                print('Avg time consumed per iteration(hh:mm:ss.ms) is {}'.format( avg_time ) )    \n",
    "                print('Time elapsed so far(hh:mm:ss.ms) is %s as of %s'%(format(time_elapsed2),datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                print('Remaiging time estimated(hh:mm:ss.ms) is %s as of %s' %(format( rem_time ),datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) )              \n",
    "  \n",
    "        # end of for iter_ii in range(0,n_iter):    \n",
    "        \n",
    "        rmse_arr_avg = np.average( rmse_arr, axis=0)  \n",
    "        rmse_arr_nsc_avg = np.average( rmse_arr_nsc, axis=0)        \n",
    "        \n",
    "        if( len(bs_set)==1 ):\n",
    "            df_rmse_arr_avg.loc[len(df_rmse_arr_avg)]         = bs_set + [-1] + list( rmse_arr_avg.flatten() ) \n",
    "            df_rmse_arr_nsc_avg.loc[len(df_rmse_arr_nsc_avg)] = bs_set + [-1] + list( rmse_arr_nsc_avg.flatten() ) \n",
    "        else: \n",
    "            df_rmse_arr_avg.loc[len(df_rmse_arr_avg)] = bs_set + list( rmse_arr_avg.flatten() ) \n",
    "            df_rmse_arr_nsc_avg.loc[len(df_rmse_arr_nsc_avg)] = bs_set + list( rmse_arr_nsc_avg.flatten() ) \n",
    "        \n",
    "        print(rmse_arr_avg)\n",
    "\n",
    "    # end of for bs_jj in range( bs_ii+1, len(bs_list_cl_sset) ):\n",
    "# end of for bs_ii in range(0, len(bs_list_cl_sset) ):    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary of test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_arr_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'df_rmse_arr_avg_' + fname_part + '_cl_' + str(cl_i) + '_setsz_' + str(bs_list_cl_sset_sz) + '_iter_' + str(n_iter)+'.csv'\n",
    "\n",
    "df_rmse_arr_avg.to_csv( fname )\n",
    "\n",
    "fname2 = 'df_rmse_arr_nsc_avg_' + fname_part + '_cl_' + str(cl_i) + '_setsz_' + str(bs_list_cl_sset_sz) + '_iter_' + str(n_iter)+'.csv'\n",
    "\n",
    "df_rmse_arr_nsc_avg.to_csv( fname2 )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
