{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Kmeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created based on cellular-05-new-weekend-modified-r6-ISD.ipynb\n",
    "\n",
    "%reset -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import neighbors\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "import statsmodels.api as sm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM \n",
    "    \n",
    "from keras import optimizers\n",
    "from keras import callbacks    \n",
    "from itertools import cycle \n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_ = 108\n",
    "seed_ = 9\n",
    "np.random.seed(seed_)  # this is numpy\n",
    "random.seed(seed_) # this is python module \n",
    "\n",
    "print( np.random.get_state()[1][0] ) # lsw) At this moment, seed_ will be printed. But then, seed will keep changing \n",
    "\n",
    "drop_col_th = 0.2 # drop columns by this ratio of NA or null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "fname_part = now.strftime(\"%Y%m%d_%H%M%S\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# series_to_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, mode_=1):\n",
    "    # n_in: Number of lag observations as input (X). Values may be between [1..len(data)] Optional. Defaults to 1.\n",
    "    # n_out: Number of observations as output (y). Values may be between [0..len(data)-1]. Optional. Defaults to 1.    \n",
    "    # mode_ = 1 => var, var2,..., else: 'user', 'bytes'....\n",
    "    \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data) # convert to a dataframe \n",
    "    cols, names = list(), list() # empty list \n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        if mode_ == 1: \n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        else: \n",
    "            names += [('%s(t-%d)' % (c_name, i)) for c_name in list(data.columns) ]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if mode_ == 1: \n",
    "            if i == 0:\n",
    "                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        else: \n",
    "            if i == 0:\n",
    "                names += [('%s(t)' % (c_name)) for c_name in list(data.columns)]\n",
    "            else:\n",
    "                names += [('%s(t+%d)' % (c_name, i)) for c_name in list(data.columns)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dto_str_wo_yr_sec(dto_in):\n",
    "    # Given a datetime object as input, output a string w/ year, sec removed\n",
    "    # input: datetime object\n",
    "    # output: string\n",
    "    \n",
    "    return dto_in.strftime( \"%m-%d %H:%M\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter(string, substr): \n",
    "    import re\n",
    "    \n",
    "    return [str for str in string if\n",
    "             any(sub in str for sub in substr)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_rem_red(list_):\n",
    "    # remove redundancy from a list, while keeping order\n",
    "    u_list = [] # unique list\n",
    "    [u_list.append(item) for item in list_ if item not in u_list]\n",
    "    \n",
    "    return u_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(list1, list2):\n",
    "    out = [item for item in list1 if not item in list2]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = float(input('Enter threshold for num_signchange(0-1):  '))\n",
    "\n",
    "def num_signchange(x):\n",
    "    # x is a series \n",
    "    \n",
    "    if x.empty:\n",
    "        return None\n",
    "    else: \n",
    "        # https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python/21171725\n",
    "        y = np.array(x)\n",
    "        y_ = np.array(y[1:]) - np.array(y[0:-1])\n",
    "        pos1 = np.where(np.diff(np.signbit( y_ )))[0] # find where sign changes occur, not the latter posion but the front position \n",
    "        pos2 = np.nonzero( np.abs(y_)> thre )[0]  # assuming minmax scaled, I assumed max 1 x 0.2 as the slope criterion\n",
    "\n",
    "        return len( np.intersect1d(pos1, pos2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Used for df_traffic_set (subset around t_bs_i), EDA, performance estimation, plot df_traffic_stat_geo\n",
    "###################################################################\n",
    "    \n",
    "adj_bs_choice = 3 \n",
    "\n",
    "max_n_adj_bs = 7 \n",
    "max_n_adj_bs_rest = -1 \n",
    "    \n",
    "input_choice = 1\n",
    "\n",
    "wknd_in_input = 2\n",
    "\n",
    "pred_target_choice = 1\n",
    "\n",
    "read_df_dist_m = 1 \n",
    "\n",
    "make_plots_or_not = 1 \n",
    "\n",
    "\n",
    "Do_EDA_hist_df_traffic = 1 \n",
    "\n",
    "Do_plot_df_topology = 1 \n",
    "\n",
    "\n",
    "Do_corr_analy_df_traffic_set = 1 \n",
    "\n",
    "\n",
    "\n",
    "Do_df_traffic_FE = 1 \n",
    "Do_df_traffic_inc_ISD = 1 \n",
    "if Do_df_traffic_inc_ISD == 1 and Do_df_traffic_FE == 1:\n",
    "    Do_df_traffic_FE_inc_ISD = 1 \n",
    "if Do_df_traffic_FE == 1: \n",
    "    Do_df_traffic_FE_inc_num_signchange = 1 # \n",
    "\n",
    "Do_df_traffic_FE2m = 1 \n",
    "\n",
    "if Do_df_traffic_FE2m == 1: \n",
    "    FE2m_ylogplot = 1 # int(input('Enter 1 to make yaxis log scale for FE2m, 2 not to:  '))\n",
    "\n",
    "DoKmeansCluster = 1 \n",
    "if DoKmeansCluster  == 1:\n",
    "    DoKmeansCluster_save = 1 \n",
    "if DoKmeansCluster in [1,2]: \n",
    "    DoKmeansCluster_xy_features = 1 \n",
    "    \n",
    "tr_ratio = 0.7 \n",
    "\n",
    "# parameter input for reframing \n",
    "n_in_time = 30 # int(input('Enter n_in_time(>1 for multiple lag input):  ')) # 1 means, (t-1) only, 2 means (t-2) and (t-1)\n",
    "n_out_time = 10 # int(input('Enter n_out_time:  ')) # 1 means t only, 2 means t and t+1\n",
    "\n",
    "use_early_stop = 2 \n",
    "    \n",
    "Do_DimRedAnaly = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch_lst = [ ['NULL'], ['users'], ['packets'], ['bytes'], ['users', 'packets'], ['users', 'bytes'], ['packets', 'bytes'], ['users', 'packets', 'bytes'] ]\n",
    "pred_tg_ch_lst = input_ch_lst.copy()\n",
    "\n",
    "input_col = 1 # input_ch_lst[input_choice].copy()\n",
    "\n",
    "\n",
    "pred_target_col = pred_tg_ch_lst[pred_target_choice]\n",
    "\n",
    "shorten_bs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_traffic and df_toplogy are read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime.datetime.fromtimestamp( int(x) ) => returns ex) datetime.datetime(2012, 8, 19, 1, 0), a datetime object \n",
    "# x: numpy.int64 object \n",
    "    \n",
    "def parse03(x):\n",
    "    return datetime.datetime.fromtimestamp( int(x) )\n",
    "        # returned example: 2012-08-19 01:00:00\n",
    "        # Otherwise, directly from the csv file, time_hour column would be like: ex) 1345305600\n",
    "        # Looks like, int(x) works same as just x \n",
    "        # datetime.fromtimestamp: Return the local date and time corresponding to the POSIX timestamp, \n",
    "        # such as is returned by...\n",
    "        # csv file's Time_hour definition: hourly timestamp in UNIX epoch time (time zone GMT+8).\n",
    "\n",
    "df_traffic = pd.read_csv(\"city-cellular-traffic-map-master/traceset/cellular_traffic_full.csv\", \\\n",
    "                     parse_dates = [1],date_parser = parse03, header=0)\n",
    "\n",
    "df_topology = pd.read_csv(\"city-cellular-traffic-map-master/traceset/topology.csv\", header = 0, index_col=0)\n",
    "    # note) longitude, latitude: usually units are in degrees # bs_i works as index    \n",
    "    \n",
    "print( df_traffic.describe() )\n",
    "print('\\n')\n",
    "print( df_topology.describe() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_dist_m, df_adj_bs generated, using df_topology\n",
    "## of bs x max_n_adj_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "# The answers above are based on the Haversine formula, which assumes the earth is a sphere, \n",
    "# which results in errors of up to about 0.5% (according to help(geopy.distance)). \n",
    "# Vincenty distance uses more accurate ellipsoidal models such as WGS-84, and is implemented in geopy. \n",
    "\n",
    "# Update: 04/2018: Note that Vincenty distance is deprecated since GeoPy version 1.13 - you should use geopy.distance.distance() instea\n",
    "\n",
    "def my_geopy_dist_m(lat1, long1, lat2, long2):\n",
    "    # return geopy.distance.vincenty([lat1,long1], [lat2, long2]).m\n",
    "    return geopy.distance.distance([lat1,long1], [lat2, long2]).m\n",
    "\n",
    "# https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "def my_geopy_dist_list2_m(lat1, long1, lat2, long2):\n",
    "    # lat1, long1, lat2, long2: should be in radians \n",
    "    R_km = 6371\n",
    "    diff_long = np.radians(long2 - long1)\n",
    "    diff_lat = np.radians(lat2 - lat1)\n",
    "    a = math.sin(diff_lat/2)**2 + math.cos(np.radians(lat1)) * math.cos(np.radians(lat2)) * math.sin(diff_long/2)**2\n",
    "    b = 2 * math.asin( np.minimum(1, np.sqrt(a))) \n",
    "    d = R_km * b\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_ = []\n",
    "for ii in range(0,max_n_adj_bs-1): # -1, since max_n_adj_bs includes center cell\n",
    "    header_ = header_ + ['adj_bs_i_' + str(ii)]\n",
    "    \n",
    "df_dist_m = pd.read_csv( 'city-cellular-traffic-map-master/traceset/df_dist_m.csv', index_col=0, header = 0)\n",
    "df_adj_bs = pd.read_csv( 'city-cellular-traffic-map-master/traceset/df_adj_bs.csv', index_col=0, header = 0)\n",
    "df_dist_m = df_dist_m.iloc[:, 0:max_n_adj_bs-1] # ex) if max_n_adj_bs=7, 0:6 ==> means neighboring 6 cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_traffic is further processed to incorporate weekend info or ISD info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean to 0/1 mapping     \n",
    "# to include ISD      \n",
    "df_traffic = pd.merge( df_traffic, df_dist_m.mean(axis=1).rename('ISDmean'), how='left', on ='bs' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_info generation for df_traffic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique =df_traffic.nunique() \n",
    "null_perc = df_traffic.isnull().sum()/df_traffic.shape[0]*100\n",
    "NA_perc = df_traffic.isnull().sum()/df_traffic.shape[0]*100\n",
    "\n",
    "df_info = pd.DataFrame({'n_unique': n_unique, 'null_perc': null_perc, 'NA_perc': NA_perc})\n",
    "df_info[\"dtyp\"]=\"\"\n",
    "df_info[\"iotyp\"]=\"\"\n",
    "\n",
    "for r_i in range(0, df_info.shape[0] ):\n",
    "    if df_info['n_unique'][r_i] >=10: # 10: just chosen \n",
    "        df_info['dtyp'][r_i] = 'numeric'\n",
    "    elif df_info['n_unique'][r_i] ==2: # don't care, whether nominal or ordinal \n",
    "        df_info['dtyp'][r_i] = 'categorical'\n",
    "    else:\n",
    "        df_info['dtyp'][r_i] = 'nominal' \n",
    "        \n",
    "    ########################################################################################\n",
    "    #  If any 'nominal sh be re-classified as 'ordinal, make your codes. OW, be default, I declared as nominal \n",
    "    ########################################################################################\n",
    "\n",
    "    df_info[\"iotyp\"][r_i]='inp'  \n",
    "        \n",
    "\n",
    "print(df_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: numeric predictors' histogram of df_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_EDA_hist_df_traffic==1:   \n",
    "    for c_i in range(0, len(df_traffic.columns) ):\n",
    "        if (df_info.loc[df_traffic.columns[c_i], 'dtyp']==\"numeric\" ): # print out, either input or output \n",
    "            if df_traffic.columns[c_i] in ['time_hour', 'bs']:\n",
    "                continue\n",
    "                \n",
    "                # imp_avg = traindata[numeric_df.iloc[f_i,0]].mean()\n",
    "                # traindata[numeric_df.iloc[f_i,0]] = traindata[numeric_df.iloc[f_i,0]].fillna(imp_avg)                \n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "            sns.distplot( df_traffic.iloc[:, c_i], fit=norm, ax=axes[0]);\n",
    "                # https://seaborn.pydata.org/generated/seaborn.distplot.html\n",
    "                # plot a univariate distribution of observations\n",
    "                # fit=norm: Plot the distribution with a histogram and maximum likelihood gaussian distribution fit:\n",
    "                    # seesm to draw the black line\n",
    "\n",
    "            res = stats.probplot( df_traffic.iloc[:, c_i] , plot=plt)\n",
    "                # check https://stackoverflow.com/questions/48108582/how-to-interpret-scipy-stats-probplot-results\n",
    "                # this shows probplot, not pp plot or qq plot\n",
    "                # Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default). probplot optionally calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\n",
    "\n",
    "                # plotobject, optiona, If given, plots the quantiles and least squares fit. \n",
    "                # res: (osm, osr) is returned \n",
    "                \n",
    "            fig.suptitle( 'histogram, df_traffic, '  + df_traffic.columns[c_i]  )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS shortening\n",
    "## df_topology, df_traffic are affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if shorten_bs == 1:\n",
    "    lon_min = df_topology.describe().loc['mean','lon']-0.5*df_topology.describe().loc['std','lon']\n",
    "    lon_max = df_topology.describe().loc['mean','lon']+0.5*df_topology.describe().loc['std','lon']\n",
    "    lat_min = df_topology.describe().loc['mean','lat']-0.5*df_topology.describe().loc['std','lat']\n",
    "    lat_max = df_topology.describe().loc['mean','lat']+0.5*df_topology.describe().loc['std','lat']\n",
    "    \n",
    "    print(df_topology.shape)\n",
    "    \n",
    "    df_topology = df_topology.loc[(df_topology['lon'] >= lon_min) & (df_topology['lon'] <= lon_max) & \\\n",
    "                    (df_topology['lat'] >= lat_min) & (df_topology['lat'] <= lat_max)]\n",
    "    \n",
    "    df_traffic = df_traffic.loc[ df_traffic.bs.isin(list(df_topology.index)) ]    \n",
    "    \n",
    "    print(df_topology.shape)\n",
    "    print('\\n')\n",
    "    print( df_traffic.describe() )\n",
    "    print('\\n')\n",
    "    print( df_topology.head(3) )\n",
    "    print('\\n')    \n",
    "    print( df_topology.describe() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shorten_bs == 1:\n",
    "    print(\"================ df_traffic.head(3) ================\")\n",
    "    print(df_traffic.head(3))\n",
    "\n",
    "    print(\"\\n================ df_traffic.shape ================\")\n",
    "    print(df_traffic.shape)\n",
    "\n",
    "    print(\"\\n================ df_traffic.dtypes ================\")\n",
    "    print(df_traffic.dtypes)\n",
    "\n",
    "    print(\"================ df_topology.head(3) ================\")\n",
    "    print(df_topology.head(3))\n",
    "\n",
    "    print(\"\\n================ df_topology.shape ================\")\n",
    "    print(df_topology.shape)\n",
    "\n",
    "    print(\"\\n================ df_topology.dtypes ================\")\n",
    "    print(df_topology.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot of df_topology at bs_set locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_plot_df_topology == 1: \n",
    "    fig, ax = plt.subplots(figsize = (8,7))\n",
    "    ax.scatter( df_topology.loc[0:2000,:].lon, df_topology.loc[0:2000,:].lat, zorder=1, alpha= 0.2, c='b', s=20)\n",
    "        \n",
    "    # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "\n",
    "    # alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "    # s: The marker size in points**2. Default is rcParams['lines.markersize'] ** 2.\n",
    "    ax.set_title('Plotting Spatial Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min max scaling on df_traffic and save as another name: df_traffic_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_perc = df_traffic.isnull().sum()/df_traffic.shape[0]*100\n",
    "print(NA_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_df_traffic_FE==1:     \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    Do_scale_FE = 1 \n",
    "\n",
    "    df_traffic_sc = df_traffic.copy().dropna(axis=0, how='any') # drop rows         \n",
    "\n",
    "    if Do_scale_FE==1: \n",
    "        col_sc = list_diff( df_traffic.columns , ['bs', 'time_hour'] ) # which columns to scale? Ans) except for 'bs' and 'time_hour'\n",
    "            \n",
    "        df_traffic_sc[ col_sc ] = scaler.fit_transform( df_traffic_sc[ col_sc ] )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check null ratio of df_traffic_sc\n",
    "null_perc = df_traffic_sc.isnull().sum()/df_traffic_sc.shape[0]*100\n",
    "NA_perc = df_traffic_sc.isnull().sum()/df_traffic_sc.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_df_traffic_FE==1:   \n",
    "    km_analy_ch_lst = [ ['NULL'], ['u-'], ['p-'], ['b-'], ['u-', 'p-'], ['u-', 'b-'], ['p-', 'b-'], ['u-', 'p-', 'b-'] ]\n",
    "    km_input_choice = int( input('For KM analysis(to generate df_traffic_FE), Input, 1(users), 2(pkt), 3(bytes), 4(users, pkt), 5(users, bytes), 6(pkt, bytes), 7(all):  ') )\n",
    "    km_input_col = km_analy_ch_lst[km_input_choice].copy()\n",
    "\n",
    "    km_input_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate df_traffic_FE using df_traffic_sc, not using melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_df_traffic_FE==1:     \n",
    "    h_vec_choice = int( input('h_vec_choice, 1: 7 intervals, 2: 4 intervals, 3: 2 intervals ') )\n",
    "\n",
    "    if h_vec_choice==1: # 7 intervals \n",
    "        h_vec = [0, 6, 9, 12, 15, 18, 21, 24]\n",
    "        h_vec__str = ['0_6', '6_9', '9_12', '12_15', '15_18', '18_21', '21_24']\n",
    "    elif h_vec_choice==2: # 4 intervals   \n",
    "        h_vec = [0, 8, 13, 18,24]    \n",
    "        h_vec_str = ['0_8', '8_13','13_18', '18_24' ]\n",
    "    elif h_vec_choice==3: # 2 intervals        \n",
    "        h_vec = [0, 8, 24]\n",
    "        h_vec_str = ['0_8', '8_24']\n",
    "\n",
    "    col_ = ['u-mean', 'u-std', 'u-min', 'u-max',\\\n",
    "            'p-mean', 'p-std', 'p-min', 'p-max', \\\n",
    "            'b-mean', 'b-std', 'b-min', 'b-max'] # fullset # Start off from fullsets \n",
    "    agg_vec = ['mean', 'std', 'min', 'max'] # aggregaton vector, start from fullset \n",
    "    daytyp_vec = ['wknd', 'wkdy'] # day type, weekend or weekday \n",
    "    stat_choice = int(input('(In generating df_traffic_FE), \\\n",
    "    Enter 1 for mean only, 2 for std only, 3 for mean and std, \\\n",
    "    4: mean, min, 5: std, min, 6: mean, std, min, 7: mean, max, 8: std, max, 9: mean, std, max,\\ \\\n",
    "    10: mean, min, max, 11: std, min, max, 12: mean, std, min, max  '))\n",
    "    \n",
    "    if stat_choice == 1:\n",
    "        Filter_by = ['mean']\n",
    "    elif stat_choice == 2:\n",
    "        Filter_by = ['std']        \n",
    "    elif stat_choice == 3:\n",
    "        Filter_by = ['mean', 'std']           \n",
    "    elif stat_choice == 4:\n",
    "        Filter_by = ['mean', 'min']                   \n",
    "    elif stat_choice == 5:\n",
    "        Filter_by = ['std', 'min']                   \n",
    "    elif stat_choice == 6:\n",
    "        Filter_by = ['mean', 'std', 'min']                   \n",
    "    elif stat_choice == 7:\n",
    "        Filter_by = ['mean','max']                   \n",
    "    elif stat_choice == 8:\n",
    "        Filter_by = ['std', 'max']                   \n",
    "    elif stat_choice == 9:\n",
    "        Filter_by = ['mean', 'std', 'max']                   \n",
    "    elif stat_choice == 10:\n",
    "        Filter_by = ['mean', 'min', 'max']                   \n",
    "    elif stat_choice == 11:\n",
    "        Filter_by = ['std', 'min', 'max']                   \n",
    "    elif stat_choice == 12:\n",
    "        Filter_by = ['mean', 'std', 'min', 'max']  \n",
    "                     \n",
    "    col_ = Filter( col_, Filter_by ) # downselect\n",
    "    agg_vec = Filter( agg_vec, Filter_by ) # downselect\n",
    "    col_ = Filter( col_, km_input_col ) # further downselect # u-, p-, b-   \n",
    "\n",
    "    for daytyp in daytyp_vec: # 2 # means expanding along xaxis \n",
    "        if wknd_in_input == 2 and daytyp == daytyp_vec[1]: # if then, doing only once is enough \n",
    "            continue \n",
    "        \n",
    "        for ii in range( 0, len(h_vec)-1 ): # 7 # means expanding along xaxis \n",
    "            cond1 = (df_traffic_sc['time_hour'].dt.hour>=h_vec[ii] )\n",
    "            cond2 = (df_traffic_sc['time_hour'].dt.hour< h_vec[ii+1])\n",
    "\n",
    "            cond3 = True\n",
    "            cond4 = True\n",
    "                \n",
    "            cond_comb = cond1 & cond2 & (cond3 | cond4)\n",
    "            temp1 = df_traffic_sc[ cond_comb ]\n",
    "\n",
    "            if km_input_choice==1:\n",
    "                temp2 = temp1.groupby('bs').agg({'users':agg_vec})\n",
    "            elif km_input_choice==2:\n",
    "                temp2 = temp1.groupby('bs').agg({'packets':agg_vec})\n",
    "            elif km_input_choice==3:\n",
    "                temp2 = temp1.groupby('bs').agg({'bytes':agg_vec})\n",
    "            elif km_input_choice==4:\n",
    "                temp2 = temp1.groupby('bs').agg({'users':agg_vec, 'packets':agg_vec})\n",
    "            elif km_input_choice==5:\n",
    "                temp2 = temp1.groupby('bs').agg({'users':agg_vec, 'bytes':agg_vec })\n",
    "            elif km_input_choice==6:\n",
    "                temp2 = temp1.groupby('bs').agg({'packets':agg_vec, 'bytes':agg_vec })\n",
    "            elif km_input_choice==7:\n",
    "                temp2 = temp1.groupby('bs').agg({'users':agg_vec, 'packets':agg_vec, 'bytes':agg_vec })\n",
    "\n",
    "            str_temp2 = \"suffix_ = ['_tz_%d_%d']*len(col_)\"%(h_vec[ii], h_vec[ii+1]) # generate suffix_\n",
    "            exec(str_temp2)\n",
    "            print('\\n')\n",
    "            print(suffix_) # ex) ['_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy']\n",
    "            temp2.columns = [m+n for m,n in zip(col_,suffix_)]\n",
    "\n",
    "            if ii==0 and daytyp == 'wknd': # if occurring for the first time \n",
    "                df_traffic_FE = temp2 # FE: feature engineering \n",
    "            else: \n",
    "                df_traffic_FE = pd.concat( [df_traffic_FE, temp2], axis = 1 )   \n",
    "                \n",
    "    if Do_df_traffic_FE_inc_num_signchange == 1: \n",
    "        for daytyp in daytyp_vec: # 2 # means expanding along xaxis \n",
    "            \n",
    "            if wknd_in_input == 2 and daytyp == daytyp_vec[1]: # if then, doing only once is enough \n",
    "                continue\n",
    "            \n",
    "            for ii in range( 0, len(h_vec)-1 ): # 7 # means expanding along xaxis \n",
    "                \n",
    "                cond1 = (df_traffic_sc['time_hour'].dt.hour>=h_vec[ii] )\n",
    "                cond2 = (df_traffic_sc['time_hour'].dt.hour< h_vec[ii+1])\n",
    "\n",
    "                cond3 = True\n",
    "                cond4 = True\n",
    "                    \n",
    "                cond_comb = cond1 & cond2 & (cond3 | cond4)\n",
    "                temp1_1 = df_traffic_sc[ cond_comb ]\n",
    "\n",
    "                if km_input_choice==1:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'users':[num_signchange]})\n",
    "                    col_1_ = ['u-nsignchg'] \n",
    "                elif km_input_choice==2:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'packets':[num_signchange]})\n",
    "                    col_1_ = ['p-nsignchg'] \n",
    "                elif km_input_choice==3:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'bytes':[num_signchange]})\n",
    "                    col_1_ = ['b-nsignchg'] \n",
    "                elif km_input_choice==4:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'users':[num_signchange], 'packets':[num_signchange]})\n",
    "                    col_1_ = ['u-nsignchg', 'p-nsignchg'] \n",
    "                elif km_input_choice==5:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'users':[num_signchange], 'bytes':[num_signchange] })\n",
    "                    col_1_ = ['u-nsignchg', 'b-nsignchg'] \n",
    "                elif km_input_choice==6:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'packets':[num_signchange], 'bytes':[num_signchange] })\n",
    "                    col_1_ = ['p-nsignchg', 'b-nsignchg'] \n",
    "                elif km_input_choice==7:\n",
    "                    temp2_1 = temp1_1.groupby('bs').agg({'users':[num_signchange], 'packets':[num_signchange], 'bytes':[num_signchange] })\n",
    "                    col_1_ = ['u-nsignchg', 'p-nsignchg', 'b-nsignchg'] \n",
    "\n",
    "                str_temp2_1 = \"suffix_1_= ['_tz_%d_%d']*len(col_)\"%(h_vec[ii], h_vec[ii+1]) # generate suffix_    \n",
    "                exec(str_temp2_1)\n",
    "                print('\\n')\n",
    "                print(suffix_1_) # ex) ['_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy', '_tz_18_24_wkdy']\n",
    "                temp2_1.columns = [m+n for m,n in zip( col_1_ ,suffix_1_)]\n",
    "                \n",
    "                if Do_scale_FE == 1:\n",
    "                    temp2_1_sc = pd.DataFrame(data = scaler.fit_transform( temp2_1.values ), index=list(temp2_1.index), columns=list(temp2_1.columns) ) \n",
    "                    temp2_1_sc.index.name = 'bs' # give name to the index column\n",
    "                    # since df_traffic_FE already exists, just keep extending\n",
    "                    df_traffic_FE = pd.merge( df_traffic_FE, temp2_1_sc, how = 'left', on='bs' )\n",
    "                else:     \n",
    "                    # since df_traffic_FE already exists, just keep extending\n",
    "                    df_traffic_FE = pd.merge( df_traffic_FE, temp2_1, how = 'left', on='bs' )\n",
    "                \n",
    "    # this does not need aggregation             \n",
    "    if Do_df_traffic_FE_inc_ISD == 1:\n",
    "        if Do_scale_FE == 1:\n",
    "            temp4 = df_traffic[['bs', 'ISDmean']].drop_duplicates(subset=['bs'], keep='first').set_index('bs')\n",
    "            temp4_sc = pd.DataFrame(data = scaler.fit_transform( temp4.values ), index=list(temp4.index), columns=list(temp4.columns) ) \n",
    "            temp4_sc.index.name = 'bs' # give name to the index column\n",
    "            df_traffic_FE = pd.merge( df_traffic_FE, \\\n",
    "                temp4_sc, how='left', on ='bs' )\n",
    "                # dropped duplicates, since having duplicates won't be good \n",
    "        else: \n",
    "            df_traffic_FE = pd.merge( df_traffic_FE, \\\n",
    "                df_traffic[['bs', 'ISDmean']].drop_duplicates(subset=['bs'], keep='first').set_index('bs'), how='left', on ='bs' )\n",
    "                    # dropped duplicates, since having duplicates won't be good \n",
    "    \n",
    "    if 1: \n",
    "        del temp1, temp2, str_temp2, col_\n",
    "        if Do_df_traffic_FE_inc_num_signchange == 1: \n",
    "            del temp1_1, temp2_1, str_temp2_1, col_1_ \n",
    "            if Do_scale_FE == 1:\n",
    "                del temp2_1_sc\n",
    "        if Do_df_traffic_FE_inc_ISD == 1:\n",
    "            del temp4, temp4_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_df_traffic_FE==1:      \n",
    "    print( df_traffic_FE.head(3) )\n",
    "    print('\\n')\n",
    "    print( df_traffic_FE.shape )\n",
    "\n",
    "    n_unique =df_traffic_FE.nunique() \n",
    "    null_perc = df_traffic_FE.isnull().sum()/df_traffic_FE.shape[0]*100\n",
    "    NA_perc = df_traffic_FE.isnull().sum()/df_traffic_FE.shape[0]*100\n",
    "\n",
    "    print(null_perc)\n",
    "    print('\\n')\n",
    "    print(NA_perc)\n",
    "    \n",
    "    ###############################################\n",
    "    # For some BS, they do not have enough samples. They may exist only for certain days, for example. \n",
    "    # If then, that BS may be fine up to df_traffic_sc, but they may have NaN from the point of df_traffic_FE \n",
    "    ###############################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data processing on df_traffic_FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_df_traffic_FE==1:  \n",
    "    df_traffic_FE = df_traffic_FE.dropna(axis = 0, how='any') # drop rows by any "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_traffic_FE2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_df_traffic_FE2m == 1: \n",
    "    for ii in range( 0, len(h_vec)-1 ): # 7\n",
    "        cond1 = (df_traffic_sc['time_hour'].dt.hour>=h_vec[ii] )\n",
    "        cond2 = (df_traffic_sc['time_hour'].dt.hour< h_vec[ii+1])\n",
    "        temp = df_traffic_sc[cond1 & cond2].drop(['time_hour'],1) \n",
    "            # temp has: bs\tusers\tpackets\tbytes\tweekend\tISDmean\tHour_info\n",
    "        \n",
    "        exec( \"Hour_info = ['Hour_%d_%d']\"%(h_vec[ii], h_vec[ii+1]) )  # ex: tz_6_9        \n",
    "        temp['Hour_info'] = Hour_info[0]\n",
    "\n",
    "        if ii==0:\n",
    "            df_traffic_FE2 = temp\n",
    "        else:\n",
    "            df_traffic_FE2 = pd.concat( [df_traffic_FE2,temp], axis=0 ) # add rows \n",
    "            \n",
    "    print( df_traffic_FE2.head(2) )      \n",
    "    \n",
    "    col_list = ['bs', 'Hour_info', 'users', 'packets', 'bytes']\n",
    "        \n",
    "    df_traffic_FE2 = df_traffic_FE2[col_list]\n",
    "\n",
    "    print('/n')\n",
    "    print( df_traffic_FE2.head(2) )\n",
    "    del col_list\n",
    "    \n",
    "\n",
    "    df_traffic_FE2m = df_traffic_FE2.melt(id_vars=[\"bs\", \"Hour_info\"], var_name=\"data_kind\", value_name=\"Value\")\n",
    "        \n",
    "    df_traffic_FE2m.head(2)\n",
    "\n",
    "    # https://seaborn.pydata.org/examples/grouped_boxplot.html\n",
    "    # https://seaborn.pydata.org/examples/grouped_boxplot.html\n",
    "    sns.set_palette(palette=\"pastel\")\n",
    "    sns.set_style(style=\"ticks\")\n",
    "\n",
    "    plt.figure()\n",
    "    # Draw a nested boxplot to show bills by day and time\n",
    "    ax = sns.boxplot(x=\"Hour_info\", y=\"Value\", hue=\"data_kind\", palette=\"colorblind\", \\\n",
    "                         data=df_traffic_FE2m, showfliers=False )    \n",
    "\n",
    "    if FE2m_ylogplot == 1: \n",
    "        ax.set(yscale=\"log\")\n",
    "    ax.set_ylim([1E-8,1])\n",
    "\n",
    "    sns.despine(offset=10, trim=True)\n",
    "        # Spines are the lines connecting the axis tick marks and noting the boundaries of the data area. They can be placed at arbitrary positions.2020. 1. 5.\n",
    "    plt.legend(loc='center right', bbox_to_anchor=(1.20, 0.5), ncol=1)\n",
    "    # ax.set_title()\n",
    "    # ax.set(ylim=(0.000000000000000000000000001, 1))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "    df_traffic_sc.describe()\n",
    "        \n",
    "################################################################################\n",
    "## If data min is 0, then boxplots may not show the minimum value line\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans clustering #1 (simple), based on df_traffic_FE\n",
    "### Then, save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DoKmeansCluster == 1:\n",
    "    Kmeans_cl_size = int( input('Enter n_clusters for Kmeans clustering: ') )\n",
    "\n",
    "    # from __future__ import print_function\n",
    "    %matplotlib inline\n",
    "        # %matplotlib inline sets the backend of matplotlib to the 'inline' backend: With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document.2\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams['figure.figsize'] = 8, 6\n",
    "    \n",
    "    kmeans = KMeans()   \n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "            # n_clustersint, optional, default: 8, The number of clusters to form as well as the number of centroids to generate.\n",
    "            # init{‘k-means++’, ‘random’ or an ndarray}, Method for initialization, defaults to ‘k-means++’:\n",
    "                # ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.\n",
    "                # ‘random’: choose k observations (rows) at random from data for the initial centroids.\n",
    "            # n_init int, default: 10, Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "            # max_iterint, default: 300, Maximum number of iterations of the k-means algorithm for a single run.\n",
    "            # random_state, int, RandomState instance or None (default), Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.\n",
    "            # algorithm“auto”, “full” or “elkan”, default=”auto”, K-means algorithm to use. \n",
    "                # The classical EM-style algorithm is “full”. The “elkan” variation is more efficient by using the triangle inequality, but currently doesn’t support sparse data. \n",
    "                # “auto” chooses “elkan” for dense data and “full” for sparse data.\n",
    "\n",
    "    kmeans.set_params(n_clusters=Kmeans_cl_size, n_init = 30, random_state = random_state_, n_jobs=None)\n",
    "    kmeans.fit(df_traffic_FE)\n",
    "    \n",
    "    if DoKmeansCluster_save == 1: \n",
    "        fname = 'kmeans_' + '_hvch_' + str(h_vec_choice) + '_clsz_' + str(Kmeans_cl_size) + '_stch_' + str(stat_choice) + '_' + fname_part + '.pkl'\n",
    "        pickle.dump(kmeans, open( fname , \"wb\"))\n",
    "            # 파이썬에서는 이와 같은 텍스트 이외의 자료형을 파일로 저장하기 위하여 pickle이라는 모듈을 제공한다.\n",
    "        del fname\n",
    "    \n",
    "    df_traffic_FE_cluster = pd.DataFrame({'bs': df_traffic_FE.index, 'cluster':kmeans.labels_ })\n",
    "    df_traffic_FE_cluster = df_traffic_FE_cluster.set_index('bs')\n",
    "    df_traffic_FE_cluster.head(2)\n",
    "    \n",
    "    if DoKmeansCluster_save == 1: \n",
    "        fname = 'df_traffic_FE' + '_hvch_' + str(h_vec_choice) + '_clsz_' + str(Kmeans_cl_size) + \\\n",
    "        '_stch_' + str(stat_choice) + '_' + fname_part + '.csv'\n",
    "        df_traffic_FE_cluster.to_csv( fname )\n",
    "        del fname\n",
    "\n",
    "elif DoKmeansCluster == 2:\n",
    "    pkl_name = input('Enter pickle file name to load:  ')\n",
    "    df_traffic_FE_cl_name = int('Enter df_traffic_FE_cluster file name to load:  ')\n",
    "    \n",
    "    # https://stackoverflow.com/questions/54879434/how-to-use-the-pickle-to-save-sklearn-model\n",
    "    kmeans = pickle.load(open(pkl_name, \"rb\"))\n",
    "    df_traffic_FE_cluster = pd.read_csv(df_traffic_FE_cl_name, header=0)\n",
    "    \n",
    "print('\\n ====== kmeans.cluster_centers_ ======')\n",
    "print(kmeans.cluster_centers_[:, 0:10])\n",
    "print(kmeans.cluster_centers_.shape)\n",
    "\n",
    "print('\\n kmeans.labels_[0:10]')\n",
    "print(kmeans.labels_[0:15])\n",
    "print( pd.Series( kmeans.labels_).value_counts() )\n",
    "\n",
    "plt.hist( kmeans.labels_, bins = np.arange(Kmeans_cl_size+1) - 0.5 ) # -0.5: for center-aligned bars\n",
    "    # https://stackoverflow.com/questions/27083051/matplotlib-xticks-not-lining-up-with-histogram\n",
    "plt.title('kmeans.labels_histogram, cluster size='+str(Kmeans_cl_size))\n",
    "plt.xticks( np.arange(Kmeans_cl_size)  ) \n",
    "plt.xlabel('cluster index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a graphical tool, called the elbow method, to estimate  𝑘 . The elbow method plots the inertia (sometimes called distortion) for different values of  𝑘#  \n",
    "\n",
    "from PlottingFunctions import plot_inertia, plot_silhouette\n",
    "plot_inertia(kmeans, df_traffic_FE, range(1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Kmeans clustering, show clusters on xy plots, uisng df_traffic_FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DoKmeansCluster in [1,2] and DoKmeansCluster_xy_features ==1: # use df_traffic_FE \n",
    "    DoKmeansCluster_xy_choice = 2 # \n",
    "    \n",
    "    # https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib\n",
    "    from itertools import cycle \n",
    "    cycol = cycle('bgrcmk')\n",
    "    # colors = ['turquoise', 'darkorange', 'darkorange', 'silver', 'steelblue'] # since Kmeans clustering with 5 \n",
    "    colors = cycle(['blue', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    \n",
    "    # kmeans_label_names = ['cl_']*Kmeans_cl_size + [str(ii) for ii in range(0, Kmeans_cl_size)]\n",
    "    kmeans_label_names = [m+n for m,n in zip(['CL_']*Kmeans_cl_size, [str(ii) for ii in range(0, Kmeans_cl_size)])]\n",
    "    # kmeans_label_names = ['clst 0', 'clst 1', 'clst 2', 'clst 3', 'clst 4'] \n",
    "    \n",
    "    # markers_ = ['o', '+', 'x', '*', 'd']\n",
    "    markers_ = ['o','s', 'x', 'd', '^']\n",
    "    markers_cyc =cycle(markers_)\n",
    "    lw = 2    # linewidth of the marker\n",
    "    alpha = 0.3 # alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "    s = 20 # marker size \n",
    "    # cmap = get_cmap( len(markers_) )\n",
    "    \n",
    "    for ind1 in range(0, df_traffic_FE.shape[1]):\n",
    "        for ind2 in range(ind1, df_traffic_FE.shape[1]):\n",
    "            if ind1 != ind2: \n",
    "                    \n",
    "                if DoKmeansCluster_xy_choice == 1: # then display only mean vs mean \n",
    "                    if '-mean' not in df_traffic_FE.columns[ind1] and '-mean' not in df_traffic_FE.columns[ind2]: \n",
    "                        continue; \n",
    "                    # lsw) since I also have 'ISDmean', skip if neigher has 'mean'\n",
    "                    \n",
    "                # if different time intervals, continue \n",
    "                if len(re.split('[-_]+', df_traffic_FE.columns[ind1])) > 1 and len(re.split('[-_]+', df_traffic_FE.columns[ind2])) > 1:\n",
    "                    # If either is of length 1, then just plot\n",
    "                    # If both are of length > 1, then just time zone sameness\n",
    "                    if re.split('[-_]+', df_traffic_FE.columns[ind1])[3]!=re.split('[-_]+', df_traffic_FE.columns[ind2])[3]:\n",
    "                        continue # visit next ind1,ind2, do not plot at this time\n",
    "                    if re.split('[-_]+', df_traffic_FE.columns[ind1])[4]!=re.split('[-_]+', df_traffic_FE.columns[ind2])[4]: \n",
    "                        continue # visit next ind1, ind2, do not plot at this time \n",
    "                \n",
    "                plt.figure()\n",
    "                ax = plt.gca()\n",
    "                \n",
    "                for i, kmeans_label_name in zip( range(0, Kmeans_cl_size), kmeans_label_names ): # i: cluster index\n",
    "                    plt.scatter(df_traffic_FE.to_numpy()[kmeans.labels_ == i, ind1], \\\n",
    "                                df_traffic_FE.to_numpy()[kmeans.labels_ == i, ind2], \\\n",
    "                                c=next(cycol), s = s, alpha=alpha , lw=lw, label=kmeans_label_name, marker = next(markers_cyc) )\n",
    "\n",
    "                    # plt.scatter(kmeans.cluster_centers_[i, ind1], kmeans.cluster_centers_[i, ind2], marker=markers_[i], s=1000, c=color )\n",
    "                        # blocked this line. If categorical values, majority vote is better. \n",
    "\n",
    "                plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "                plt.title( 'After Kmeans clustering' )  \n",
    "                plt.xlabel( df_traffic_FE.columns[ind1] )\n",
    "                plt.ylabel( df_traffic_FE.columns[ind2] )\n",
    "                \n",
    "                # ax.set_xscale('log')\n",
    "                # ax.set_yscale('log')\n",
    "                \n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Do_DimRedAnaly == 1: \n",
    "    # copied from Computer_Vision_Lab-Radionom-liv-r31-loop.ipynb\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.set_params(n_components = None, random_state = random_state_ ) # in the beginning, do not restrct by n_compo\n",
    "    pca.fit( df_traffic_FE )    \n",
    "    \n",
    "    n_compo = int(input('Enter n_compo for PCA analysis:  '))\n",
    "\n",
    "    # plt.plot(range(30), pca.explained_variance_ratio_[0:30])\n",
    "    plt.plot(pca.explained_variance_ratio_[0:30] )\n",
    "    plt.xlabel('ith components')\n",
    "    plt.ylabel('Percentage of Variance')\n",
    "    plt.title('PCA analysis')\n",
    "    \n",
    "    plt.xticks(np.arange(0, 30, step = 1))  # S\n",
    "    \n",
    "    plt.show()\n",
    "    print('\\n pca.explained_variance_ratio_ is(n_components=None)[0:30]: ')\n",
    "    print( pca.explained_variance_ratio_[0:30] )\n",
    "    print('\\n total percentage of variance explainedn_components=None)[0:30]: ')\n",
    "    print( np.sum(pca.explained_variance_ratio_[0:30])  )  \n",
    "\n",
    "    pca.set_params( n_components=n_compo, random_state = random_state_ ).fit( df_traffic_FE )\n",
    "    \n",
    "    df_traffic_FE_PCA = pd.DataFrame( data = pca.transform(df_traffic_FE), index = df_traffic_FE.index, \\\n",
    "                                     columns = ['component  ' + str(i) for i in range(1,n_compo+1)] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DoKmeansCluster in [1,2] and DoKmeansCluster_xy_features ==1 and Do_DimRedAnaly == 1: # use df_traffic_FE \n",
    "    # https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib\n",
    "    from itertools import cycle \n",
    "    \n",
    "    # cycol = cycle('bgrcmk')\n",
    "    # colors = ['turquoise', 'darkorange', 'darkorange', 'silver', 'steelblue'] # since Kmeans clustering with 5 \n",
    "    \n",
    "    # kmeans_label_names = ['cl_']*Kmeans_cl_size + [str(ii) for ii in range(0, Kmeans_cl_size)]\n",
    "    kmeans_label_names = [m+n for m,n in zip(['CL_']*Kmeans_cl_size, [str(ii) for ii in range(0, Kmeans_cl_size)])]\n",
    "    # kmeans_label_names = ['clst 0', 'clst 1', 'clst 2', 'clst 3', 'clst 4'] \n",
    "    \n",
    "    # markers_ = ['o', '+', 'x', '*', 'd']\n",
    "\n",
    "    lw = 2    # linewidth of the marker\n",
    "    alpha = 0.8 # alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "    s = 15 # marker size \n",
    "    # cmap = get_cmap( len(markers_) )\n",
    "    \n",
    "    for ind1 in range(0, df_traffic_FE_PCA.shape[1]):\n",
    "        for ind2 in range(ind1, df_traffic_FE_PCA.shape[1]):\n",
    "            if ind1 != ind2: \n",
    "                \n",
    "                colors = ['blue', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']\n",
    "                colors_cyc = cycle(colors)\n",
    "\n",
    "                markers_ = ['o','s', 'x', 'd', '^']\n",
    "                markers_cyc =cycle(markers_)    \n",
    "                \n",
    "                plt.figure()\n",
    "                ax = plt.gca()\n",
    "                \n",
    "                for i, kmeans_label_name in zip( range(0, Kmeans_cl_size), kmeans_label_names ): # i: cluster index\n",
    "                    plt.scatter( df_traffic_FE_PCA[kmeans.labels_ == i].iloc[:, ind1], \\\n",
    "                                 df_traffic_FE_PCA[kmeans.labels_ == i].iloc[:, ind2], \\\n",
    "                                 c=next(colors_cyc), s = s, alpha=alpha , lw=lw, label=kmeans_label_name, \\\n",
    "                                 marker = next(markers_cyc) )\n",
    "\n",
    "                    # plt.scatter(kmeans.cluster_centers_[i, ind1], kmeans.cluster_centers_[i, ind2], marker=markers_[i], s=1000, c=color )\n",
    "                        # blocked this line. If categorical values, majority vote is better. \n",
    "\n",
    "                plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "                plt.title( 'Kmeans clustering displayed by PCA of ' + str(df_traffic_FE_PCA.shape[0]) + ' BS' )  \n",
    "                plt.xlabel( 'PCA ' + str(ind1+1) + 'st component' )\n",
    "                plt.ylabel( 'PCA ' + str(ind2+1) + 'st component' )\n",
    "                \n",
    "                # ax.set_xscale('log')\n",
    "                # ax.set_yscale('log')\n",
    "                \n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot of df_topology at bs_list locations(based on df_traffic_FE or df_traffic_FE_cluster) \n",
    "# with Kmeans clustering colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text = int(input('Enter 1 to display text, 2 not to:  '))\n",
    "\n",
    "if Do_plot_df_topology == 1: \n",
    "    cycol = cycle('bgrcmk')\n",
    "    plt.figure( figsize=(12,8) )\n",
    "    \n",
    "    # colors = cycle(['cyan', 'tomato', 'dodgerblue', 'lawngreen', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    # colors = cycle(['fuchsia', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    colors = cycle(['blue', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    \n",
    "    # markers_ = ['o', '+', 'x', '*', 'd']\n",
    "    markers_ = ['o','s', 'x', 'd', '^']\n",
    "    markers_cyc =cycle(markers_)    \n",
    "\n",
    "    # all base stations\n",
    "    for cl_i in range(0, Kmeans_cl_size): # 0...6\n",
    "        bs_list = list(df_traffic_FE_cluster[ df_traffic_FE_cluster['cluster']==cl_i ].index)\n",
    "\n",
    "        plt.scatter( df_topology.loc[ bs_list, : ].lon, df_topology.loc[ bs_list, : ].lat, zorder=1, alpha= 0.7, c=next(colors), s=15, label='CL '+str(cl_i) )\n",
    "        \n",
    "        # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "        # alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "        # s: The marker size in points**2. Default is rcParams['lines.markersize'] ** 2.\n",
    "        \n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.title( 'Locations of ' + str(len( df_traffic_FE_cluster )) + ' base stations' )  \n",
    "    plt.xlabel('longitude')\n",
    "    plt.ylabel('latitude')\n",
    "    plt.show()        \n",
    "    \n",
    "    ######################################################################################    \n",
    "    ################ specific area only display, all clusters, with texts ################\n",
    "    ######################################################################################\n",
    "    \n",
    "    # colors = cycle(['cyan', 'tomato', 'dodgerblue', 'lawngreen', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    # colors = cycle(['fuchsia', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    colors = cycle(['blue', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "    \n",
    "    # markers_ = ['o', '+', 'x', '*', 'd']\n",
    "    markers_ = ['o','s', 'x', 'd', '^']\n",
    "    markers_cyc =cycle(markers_)     \n",
    "    \n",
    "    lon_min_sh = 111.071\n",
    "    lon_max_sh = 111.079\n",
    "    lat_min_sh = 13.121\n",
    "    lat_max_sh = 13.129\n",
    "            \n",
    "    \n",
    "    df_topology_sh = df_topology.loc[(df_topology['lon'] >= lon_min_sh) & (df_topology['lon'] <= lon_max_sh) & \\\n",
    "                    (df_topology['lat'] >= lat_min_sh) & (df_topology['lat'] <= lat_max_sh)]\n",
    "\n",
    "    lon_d_sh = lon_max_sh-lon_min_sh\n",
    "    lat_d_sh = lat_max_sh-lat_min_sh \n",
    "\n",
    "    lon_d_sh_text = lon_d_sh/240 # to be used for text display\n",
    "    lat_d_sh_text = lat_d_sh/180 # to be used for text display\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    num_BS = 0 \n",
    "\n",
    "    for cl_i in range(0, Kmeans_cl_size): # 0...6\n",
    "        # generated for each cl_i\n",
    "\n",
    "        bs_list_sh = set.intersection( set(df_traffic_FE_cluster[ df_traffic_FE_cluster['cluster']==cl_i ].index), set(df_topology_sh.index) )\n",
    "        \n",
    "        num_BS +=len(bs_list_sh)\n",
    "\n",
    "        plt.scatter( df_topology_sh.loc[ bs_list_sh, : ].lon, df_topology_sh.loc[ bs_list_sh, : ].lat, \\\n",
    "                    zorder=1, alpha= 0.9, c=next(colors), marker = next(markers_cyc), s=100, label='CL '+str(cl_i) )\n",
    "\n",
    "        # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "        # alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "        # s: The marker size in points**2. Default is rcParams['lines.markersize'] ** 2.\n",
    "\n",
    "        if plot_text == 1: \n",
    "            for ii, bb, lon_, lat_ in zip( range(0,len(bs_list_sh)) , bs_list_sh, df_topology_sh.loc[bs_list_sh,:].lon, df_topology_sh.loc[bs_list_sh,:].lat ):\n",
    "                plt.text(lon_+lon_d_sh_text, lat_+lat_d_sh_text, str(cl_i)+','+str(bb), fontsize=11, rotation = 20)      \n",
    "\n",
    "    # plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.legend(loc='upper left', shadow=False, scatterpoints=1, bbox_to_anchor=(1.02, 1), fontsize='x-large', \\\n",
    "               numpoints = 15)\n",
    "        # https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot\n",
    "    plt.title( 'Locations of ' + str(num_BS) + ' base stations' )  \n",
    "    plt.xlabel('longitude')\n",
    "    plt.ylabel('latitude')\n",
    "\n",
    "    plt.axis([lon_min_sh-lon_d_sh/20, lon_max_sh+lon_d_sh/20, lat_min_sh-lat_d_sh/20, lat_max_sh+lat_d_sh/20])    \n",
    "\n",
    "    \n",
    "    # del bs_list_sh\n",
    "    # del df_topology_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DoKmeansCluster == 1:\n",
    "    df_traffic_FE_cluster_sh = df_traffic_FE_cluster.loc[ set.intersection( set(df_topology_sh.index), set(df_traffic_FE_cluster.index) ) ] # slice out accordding the bs indexes\n",
    "    # set.intersection: otherwise, due to NA, df_traffic_FE_cluster. has less # of BSs than df_topology_sh.index\n",
    "    \n",
    "    if DoKmeansCluster_save == 1: \n",
    "        fname = 'df_traffic_FE_sh' + '_hvch_' + str(h_vec_choice) + '_clsz_' + str(Kmeans_cl_size) + \\\n",
    "        '_stch_' + str(stat_choice) + '_' + fname_part + '.csv'   \n",
    "        df_traffic_FE_cluster_sh.to_csv( fname )\n",
    "        \n",
    "        del fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA analysis display for only certain BSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DoKmeansCluster in [1,2] and DoKmeansCluster_xy_features ==1 and Do_DimRedAnaly == 1: # use df_traffic_FE \n",
    "    # https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib\n",
    "    \n",
    "    text_disp = int(input('Enter 1 to display CL and BS index, 2 not to: :  '))\n",
    "    \n",
    "    xx_d_text = 2/500 # to be used for text display # I think PCA is -1 ~ 1\n",
    "    yy_d_text = 2/500 # to be used for text display # I think PCA is -1 ~ 1\n",
    "    \n",
    "    alpha = 0.7 # alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "    s = 50 # marker size \n",
    "    \n",
    "    for ind1 in range(0, df_traffic_FE_PCA.shape[1]): # x axis\n",
    "        for ind2 in range(ind1, df_traffic_FE_PCA.shape[1]): # y axis \n",
    "            \n",
    "            colors = cycle(['blue', 'tomato', 'dodgerblue', 'turquoise', 'magenta', 'indigo', 'darkgray']) # since Kmeans clustering with 5     \n",
    "            markers_ = ['o','s', 'x', 'd', '^']\n",
    "            markers_cyc =cycle(markers_)  \n",
    "            \n",
    "            if ind1 != ind2: \n",
    "                plt.figure(figsize=(20,10))\n",
    "                ax = plt.gca()\n",
    "                \n",
    "                for cl_i, kmeans_label_name in zip( range(0, Kmeans_cl_size), kmeans_label_names ): # i: cluster index\n",
    "                    temp1 = df_traffic_FE_PCA[kmeans.labels_ == cl_i] # subselect by true/false of same length # temp1 still has bs as index\n",
    "                    temp2 = temp1.loc[ list( set.intersection( set(temp1.index), set(df_topology_sh.index) ) ) ]\n",
    "                    bs_list_sh = list( temp2.index ) # BS index specific for shortened list & cl_i\n",
    "                    \n",
    "                    # df_topology_sh.index is also bs index\n",
    "                    # loc: by 'index' column\n",
    "                    \n",
    "                    plt.scatter( temp2.iloc[:,ind1],temp2.iloc[:,ind2], \\\n",
    "                                c=next(colors), s = s, alpha=alpha , lw=lw, label=kmeans_label_name, \\\n",
    "                                marker = next(markers_cyc) )\n",
    "                    # plt.scatter(kmeans.cluster_centers_[i, ind1], kmeans.cluster_centers_[i, ind2], marker=markers_[i], s=1000, c=color )\n",
    "                        # blocked this line. If categorical values, majority vote is better. \n",
    "                        \n",
    "                    if text_disp == 1:     \n",
    "                        for ii, bb, xx, yy, in zip( range(0,len(bs_list_sh)) , bs_list_sh, \\\n",
    "                                df_traffic_FE_PCA.loc[bs_list_sh].iloc[:, ind1], df_traffic_FE_PCA.loc[bs_list_sh].iloc[:, ind2] ):\n",
    "                            plt.text(xx+xx_d_text, yy+yy_d_text, str(cl_i)+','+str(bb), fontsize = 12, rotation = 20)      \n",
    "\n",
    "                plt.legend(loc='upper left', shadow=False, scatterpoints=1, bbox_to_anchor=(1.02, 1), fontsize='large', \\\n",
    "                   numpoints = 15)        \n",
    "                    # plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "                plt.title( 'After Kmeans clustering of ' + str(num_BS) + ' BS' )  \n",
    "                plt.xlabel( 'PCA ' + str(ind1+1) + 'st compo' )\n",
    "                plt.ylabel( 'PCA ' + str(ind2+1) + 'st compo' )\n",
    "                \n",
    "                # ax.set_xscale('log')\n",
    "                # ax.set_yscale('log')\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "    del temp1\n",
    "    del temp2\n",
    "    # del aa, bb, cc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
